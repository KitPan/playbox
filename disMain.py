'''The purpose of this program is to find some optimal parameter
   settings for a given program. If you expose parameters to the
   command line, but don't know what the optimal default value
   should be, this program will distribute the parameter
   possibilities over a cluster of machines, and rank the results
   based on user defined criteria.

   dispynode.py must be run on all machines in the cluster prior to
   running this program. The machines will be identified
   automatically, and made available for processing.

   The user must specify a program, which is assumed to be network
   accessible to all nodes of the cluster. The user must also specify
   a argument file. This file is assumed to be a text file with all
   of the parameter sets the user wants to test, one per line. The
   program will handle distribution over the cluster, and returning
   the results. 
   
   If the user needs additional functionality, they can derive the
   JobRunner class with some additional functionality specific to 
   their needs.
'''
import pickle

class JobRunner() :
    
    def __init__(self, jobID, prog, argList) :
        '''Setup some common information for the class. This object is
           distributed with the job to the target node on the cluster. It
           is used as a conduit for passing information around the cluster
           and collecting results later.

           prog     - program to run (assumed to be network accessible)
           argList  - unique list of arguments to run on this node
           jobID    - unique identifier for this job (numeric)
           hostname - name of the machine this job was ran on
           stdout   - standard output generated by the subprocess
           stderr   - standard error generated by the subprocess
           results  - results should contain a metric for which the host 
                      can later rank the job results.
        '''
        self.prog = prog
        self.argList = argList if not isinstance(argList, str) \
                       else argList.split()
        self.jobID = jobID
        self.hostname = None
        self.stdout = None
        self.stderr = None
        self.result = None

    # its important to have a __str__ so we can print to the logger later
    def __str__(self) :
        return 'Job [' + str(self.jobID) + ']\n' + \
               '\tProgram  = ' + str(self.prog) + '\n' + \
               '\tArgList  = ' + str(self.argList) + '\n' + \
               '\tHostName = ' + str(self.hostname) + '\n' + \
               '\tStdOut   = ' + str(self.stdout.strip()) + '\n' + \
               '\tStdErr   = ' + str(self.stderr.strip()) + '\n' + \
               '\tResult   = ' + str(self.result) + '\n'

    # run this command as a subprocess on the node
    def _runSubProcess(self) :
        from subprocess import Popen, PIPE
        import socket
        self.hostname = socket.gethostname()
        (self.stdout, self.stderr) = Popen(args=[self.prog] + self.argList,
                                           stdout=PIPE, stderr=PIPE,
                                           shell=False).communicate()

    # collect the results for this run
    def _collectResults(self) :
        # by default we assume the program returns a numeric metric
        # for ranking to standard output
        self.result = float(self.stdout)
        
    def run(self) :
        '''Override this method if you need the nodes to perform a different operation.'''
        self._runSubProcess()
        self._collectResults()

    @staticmethod
    def rank(runners) :
        '''Override this method if you need a different ranking scheme.'''
        lowestRunner = None
        for runner in runners :
            if lowestRunner is None or runner.result < lowestRunner.result :
                lowestRunner = runner
        return lowestRunner

def disCompute(runner) :
    '''This method gets sent out to the node in order to run the request.
       memory updated here is lost, so we play the trick of updating the
       internal members of the runner class, and then return it as the
       result. This is pickled, so it gets serialized appropriately.
    '''
    # run the provided class
    runner.run()

    # pickle the object and send it back
    return pickle.dumps(runner)

if __name__ == '__main__':
    import dispy, dispy.httpd, argparse, logging, sys
    from cStringIO import StringIO

    parser = argparse.ArgumentParser()
    parser.add_argument('--prog', dest='prog',
                        help='Program to run in the subprocess.')
    parser.add_argument('--args', dest='args', 
                        help='File containing all program variations (one per line)')
    parser.add_argument('--override', dest='jobRunner', 
                        help='Python file containing a derived JobRunner')
    options = parser.parse_args()

    # setup a typedef for the new runner    
    JR = JobRunner
    if options.jobRunner is not None :
        cls = options.jobRunner
        imp = __import__(cls)
        JR = imp.cls

    # setup the logger
    log = logging.getLogger('dispy: ' + options.prog)
    log.setLevel(logging.INFO)
    formatter = logging.Formatter('%(levelname)s - %(message)s')
    stream = logging.StreamHandler()
    stream.setLevel(logging.DEBUG)
    stream.setFormatter(formatter)
    log.addHandler(stream)

    # open the argument list for parsing
    with open(options.args, 'r') as f :
        argSets = f.readlines()

    # create the cluster for this program launch
    cluster = dispy.JobCluster(disCompute, depends=[JobRunner])

    # start a monitor for the cluster at http://localhost:8181
    jobMonitor = dispy.httpd.DispyHTTPServer(cluster, host='localhost')

    # submit the jobs to the cluster --
    # this submits each argument set individually, and stores the jobs
    # so we can later recall their outputs and status. There is no
    # assignment here to schedule on the next available node.
    jobs = []
    for ii in range(len(argSets)) :
        log.info('Adding Job[' + str(ii) + ']')

        # create a runner to perform the work on each node
        runner = JobRunner(ii, options.prog, argSets[ii])

        # this adds the job to the queue, and its input is the runner
        job = cluster.submit(runner)
        job.id = ii
        jobs.append(job)

    # wait for the cluster to finish
    cluster.wait()

    # print the statistic of the clustered run -- Hacktacular!
    # We have no access to their method's print, so we temporarily redirect
    # stdout to a string, so we can capture the status in our logger.
    temp = sys.stdout
    sys.stdout = StringIO()
    cluster.stats()
    log.info(sys.stdout.getvalue())
    sys.stdout = temp   

    # deserialize the pickled results back into our runners
    runners = []
    for job in jobs :
        runner = pickle.loads(job.result)
        runners.append(runner)
        log.debug('Job[' + str(job.id) + ']: ' + str(runner))
        log.debug('Job[' + str(job.id) + ']: ' + str(job.stdout))
        log.debug('Job[' + str(job.id) + ']: ' + str(job.stderr))

    # rank the result and return the best
    bestJob = JR.rank(runners)
    log.info('The best job was: \n' + str(bestJob))

    # cleanup
    jobMonitor.shutdown()
    cluster.close()